name: Flutter Test

on:
  workflow_call:
    inputs:
      analyze_directories:
        required: false
        type: string
        default: "lib test"
      flutter_channel:
        required: false
        type: string
        default: "stable"
      flutter_version:
        required: false
        type: string
        default: ""
      format_directories:
        required: false
        type: string
        default: "lib test"
      format_line_length:
        required: false
        type: string
        default: "80"
      runs_on:
        required: false
        type: string
        default: "ubuntu-latest"
      working_directory:
        required: false
        type: string
        default: "."
      continue_on_error:
        required: false
        type: boolean
        default: false
    secrets:
      dot_env:
        required: false
    outputs:
      format_result: 
        description: "The result of the format check"
        value: ${{ jobs.deveji-flutter-test.outputs.format_result }}
      analyze_result:
        description: "The result of the analyze check"
        value: ${{ jobs.deveji-flutter-test.outputs.analyze_result }}

      passed_tests:
        description: "The number of passed tests"
        value: ${{ jobs.deveji-flutter-test.outputs.passed_tests }}
      failed_tests:
        description: "The number of failed tests"
        value: ${{ jobs.deveji-flutter-test.outputs.failed_tests }}

jobs:
  deveji-flutter-test:
    defaults:
      run:
        working-directory: ${{inputs.working_directory}}
    outputs:
      format_result: ${{ steps.format.outputs.CHECK_FORMAT }}
      analyze_result: ${{ steps.analyze.outputs.CHECK_ANALYZE }}
      passed_tests: ${{ steps.test.outputs.PASSED_TESTS }}
      failed_tests: ${{ steps.test.outputs.FAILED_TESTS }}

      

    runs-on: ${{inputs.runs_on}}

    steps:
      - name: üìö Git Checkout
        uses: actions/checkout@v4.1.6

      - name: üê¶ Setup Flutter
        uses: subosito/flutter-action@v2.16.0
        with:
          flutter-version: ${{inputs.flutter_version}}
          channel: ${{inputs.flutter_channel}}
          cache: true
          cache-key: flutter-:os:-:channel:-:version:-:arch:-:hash:-${{ hashFiles('**/pubspec.lock') }}

      - name: üìù Add .env file
        run: echo "${{ secrets.dot_env }}" > .env

      - name: üì¶ Install Dependencies
        run: |
          flutter clean
          dart pub global activate build_runner
          dart run build_runner build -d
          flutter pub get


      - name: ‚ú® Check Formatting
        id: format
        run: |
          dart format --line-length ${{inputs.format_line_length}} --set-exit-if-changed ${{inputs.format_directories}}
          echo "test format"
          check=$?
          echo "CHECK_FORMAT=$check" >> $GITHUB_ENV
          echo "CHECK_FORMAT=$check" >> $GITHUB_OUTPUT
          exit $check
        continue-on-error: true

      - name: üïµÔ∏è Analyze
        id: analyze
        run: |
          flutter analyze ${{inputs.analyze_directories}}
          echo "test analyze"
          check=$?
          echo "CHECK_ANALYZE=$check" >> $GITHUB_ENV
          echo "CHECK_ANALYZE=$check" >> $GITHUB_OUTPUT
          exit $check
        continue-on-error: true

      - name: üß™ Run Tests
        id: test
        run: | 
          output=$(flutter test --test-randomize-ordering-seed random --reporter github| tee /dev/fd/2)
          # Strip ::error:: from the output
          temp_output=$(echo "$output" | sed 's/::error:://g')
          
          passed_tests=0
          failed_tests=0
          
          if echo "$temp_output" | grep -q 'tests passed,'
          then
            # If some tests failed
            passed_tests=$(echo "$temp_output" | awk -F' ' '/tests passed,/ {print $1}')
            failed_tests=$(echo "$temp_output" | awk -F' ' '/tests passed,/ {print $4}')
          else
            # If all tests passed
            passed_tests=$(echo "$temp_output" | awk -F' ' '/tests passed./ {print $2}')
          fi
          passed_tests_percentage=$(echo "scale=2; $passed_tests*100/($passed_tests+$failed_tests)" | bc)

          
          echo "Tests passed: $passed_tests"
          echo "Tests failed: $failed_tests"
          echo "Test result: $failed_tests"
          echo "Percentage: $passed_tests_percentage %"

          echo "PASSED_TESTS=$passed_tests" >> $GITHUB_OUTPUT
          echo "FAILED_TESTS=$failed_tests" >> $GITHUB_OUTPUT
          echo "PASSED_TESTS=$passed_tests" >> $GITHUB_ENV
          echo "FAILED_TESTS=$failed_tests" >> $GITHUB_ENV
          exit $failed_tests
        continue-on-error: true
      
      - name: üöÄ Results
        run: |
          echo "### üìù Check Formatting: $CHECK_FORMAT"
          echo "### üïµÔ∏è Analyze: $CHECK_ANALYZE"
          echo "### üß™ Test: $FAILED_TESTS"
          if [[ "$CHECK_FORMAT" != "0" ]]; then
            echo "### ‚ùå Formatting check failed!" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚úÖ Formatting check passed!" >> $GITHUB_STEP_SUMMARY
          fi
          if [[ "$CHECK_ANALYZE" != "0" ]]; then
            echo "### ‚ùå Analyze check failed!" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚úÖ Analyze check passed!" >> $GITHUB_STEP_SUMMARY
          fi
          if [[ "$FAILED_TESTS" != "0" ]]; then
            echo "### ‚ùå Test check failed!" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚úÖ Test check passed!" >> $GITHUB_STEP_SUMMARY
          fi
          if [[ "$CHECK_FORMAT" == "0" ]] && [[ "$CHECK_ANALYZE" == "0" ]] && [[ "$FAILED_TESTS" == "0" ]]; then
            echo "### üéâ All checks passed! :rocket:" >> $GITHUB_STEP_SUMMARY
          fi
        continue-on-error: true

      - name: üèÅ Exit
        env:
          OUTPUT_CHECK_FORMAT: ${{ steps.format.outputs.CHECK_FORMAT }}
          OUTPUT_CHECK_ANALYZE: ${{ steps.analyze.outputs.CHECK_ANALYZE }}
          OUTPUT_PASSED_TESTS: ${{ steps.test.outputs.passed_tests }}
          OUTPUT_FAILED_TESTS: ${{ steps.test.outputs.failed_tests }}
        run: |
          echo "Format results: $OUTPUT_CHECK_FORMAT"
          echo "Analyze results: $OUTPUT_CHECK_ANALYZE"

          echo "Passed tests: $OUTPUT_PASSED_TESTS"
          echo "Failed tests: $OUTPUT_FAILED_TESTS"
          if [[ "${{inputs.continue_on_error}}" == "false" ]]; then
            exit $OUTPUT_FAILED_TESTS
          else
            exit 0
          fi